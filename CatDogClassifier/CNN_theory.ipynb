{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "$$ f * g = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) \\ d\\tau$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "This process tries to find the distinctive features of an image, and creates a reduced-size version of the image by eliminating the irrelevant information.\n",
    "There are three main benefits:\n",
    "- _reduce overfitting_, because the irrelevant information was cut out\n",
    "- it helps to achive _spacial invariance_, which means that no matter how stretched or which orientation the image would take, it'll be recognized by the CNN  \n",
    "- _reduces the amount of computations_ needed to classify this image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattering\n",
    "This is done to fit the images of the pooling layer, which are in an $n \\times m$ shape, into an input layer of the Artificial Neural Network, which is required to be in $ 1 \\times n\\cdot m $ shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "This function is used to ensure that the sumation of all the probabilities returned by the output layer would be equal to 1. In other words, it creates a probability space were the output with the bigest value of $z_i$ is the class with the highest chances of being the correct answer.\n",
    "\n",
    "Here it is the equation that represents the softmax activation function\n",
    "$$ f_j (z) = \\frac{e^{zj}}{\\sum_{k=1}^{N} e^{zk}} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "It's used as loss function for CNNs in classification problems, which works better than Mean Square Error (MSE).\n",
    "The equation of the cross-entropy is\n",
    "$$ L_i = -\\log \\bigg( \\frac{e^{f_{y_i}}}{\\sum_{j=1}^{N} e^{f_j}} \\bigg)$$\n",
    "but for simplicity and to adress the main concept let's use the classic definition of entropy in an information system  \n",
    "$$ H(p,q) = - \\sum_{x=1}^{N} p(x)\\log q(x) $$\n",
    "where $p$ is the label and in $q$ is the output with the highest probability "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
