{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmBIvLVMcS7"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1aP4g1CQC8T"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPY-DxZGQMyz"
      },
      "source": [
        "### Importing the libraries \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "87UHHCltStwi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk10lak7QRBq"
      },
      "source": [
        "## Part 1 - Data Preprocessing (image augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ReqoqTQdQ0"
      },
      "source": [
        "### Preprocessing the training set\n",
        "- rescale: _feature scalling_ because that's the representation range, this is a normalization \n",
        "- shear_range: is\n",
        "- zoom_range = applies zoom over the images\n",
        "- horizontal_flip = \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N5mk3xuYUCyV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255, \n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "    'C:/Users/Asus/Desktop/programitas/CatDogClassifier/dataset/dataset/training_set',\n",
        "    target_size = (64,64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPXzZtCUR3_N"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4HQjxb5xibt2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    'C:/Users/Asus/Desktop/programitas/CatDogClassifier/dataset/dataset/test_set',\n",
        "    target_size = (64,64),\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0K9wrcQed4"
      },
      "source": [
        "## Part 2 - Building the CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mG-cywTQwSj"
      },
      "source": [
        "### Initialising the CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IehiDEl3jaBr"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_KFS36RQ2DO"
      },
      "source": [
        "### Setp 1 - Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Hvq7pw6NjhdL"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size= 3, activation = 'relu',input_shape=[64,64,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Y5g201Q4hz"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h9T5eymWkJ58"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6X7W0oXQ7_e"
      },
      "source": [
        "### Adding a second convolutional layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8DqXJ3fDkpC7"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size= 3, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYLewwWRCFl"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-ResI9j3lCCR"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT0TTbhGRFW_"
      },
      "source": [
        "### Step 4 - Full Connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CbfN8vxWlJSx"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfvdDKcRW9N"
      },
      "source": [
        "### Step 5 - Output Layer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ayZGueEllaQ2"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWkAByUJRW0O"
      },
      "source": [
        "## Part 3 - Training the CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3b1zC13Regw"
      },
      "source": [
        "### Compiling the CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GGEXJQwqlr8B"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF4VvtMLRiyA"
      },
      "source": [
        "### Training the CNN on the training set an evaluation it on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dXKNdppwl-44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 93s 367ms/step - loss: 0.6648 - accuracy: 0.5911 - val_loss: 0.6395 - val_accuracy: 0.6425\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.6069 - accuracy: 0.6693 - val_loss: 0.5980 - val_accuracy: 0.6840\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.5653 - accuracy: 0.7056 - val_loss: 0.5435 - val_accuracy: 0.7355\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.5208 - accuracy: 0.7368 - val_loss: 0.5128 - val_accuracy: 0.7435\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 45s 182ms/step - loss: 0.5031 - accuracy: 0.7511 - val_loss: 0.4856 - val_accuracy: 0.7760\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 45s 180ms/step - loss: 0.4897 - accuracy: 0.7563 - val_loss: 0.5158 - val_accuracy: 0.7440\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 40s 159ms/step - loss: 0.4664 - accuracy: 0.7760 - val_loss: 0.5131 - val_accuracy: 0.7485\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 40s 162ms/step - loss: 0.4566 - accuracy: 0.7839 - val_loss: 0.4526 - val_accuracy: 0.7900\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 43s 172ms/step - loss: 0.4329 - accuracy: 0.7979 - val_loss: 0.4651 - val_accuracy: 0.7930\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 43s 173ms/step - loss: 0.4201 - accuracy: 0.8044 - val_loss: 0.4666 - val_accuracy: 0.7810\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.3996 - accuracy: 0.8161 - val_loss: 0.5043 - val_accuracy: 0.7735\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.3870 - accuracy: 0.8224 - val_loss: 0.5649 - val_accuracy: 0.7220\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3743 - accuracy: 0.8285 - val_loss: 0.4514 - val_accuracy: 0.7975\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 44s 178ms/step - loss: 0.3591 - accuracy: 0.8371 - val_loss: 0.4484 - val_accuracy: 0.7990\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 45s 180ms/step - loss: 0.3461 - accuracy: 0.8509 - val_loss: 0.4875 - val_accuracy: 0.7925\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.3305 - accuracy: 0.8534 - val_loss: 0.4693 - val_accuracy: 0.8065\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3155 - accuracy: 0.8633 - val_loss: 0.4536 - val_accuracy: 0.8060\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 45s 180ms/step - loss: 0.3037 - accuracy: 0.8668 - val_loss: 0.4941 - val_accuracy: 0.7900\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.2827 - accuracy: 0.8784 - val_loss: 0.4893 - val_accuracy: 0.8030\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 44s 177ms/step - loss: 0.2775 - accuracy: 0.8834 - val_loss: 0.4832 - val_accuracy: 0.7985\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 45s 180ms/step - loss: 0.2633 - accuracy: 0.8919 - val_loss: 0.5546 - val_accuracy: 0.7875\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 47s 188ms/step - loss: 0.2554 - accuracy: 0.8907 - val_loss: 0.5336 - val_accuracy: 0.7930\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 47s 188ms/step - loss: 0.2395 - accuracy: 0.9006 - val_loss: 0.4842 - val_accuracy: 0.8055\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 46s 185ms/step - loss: 0.2356 - accuracy: 0.9019 - val_loss: 0.5256 - val_accuracy: 0.8065\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 47s 188ms/step - loss: 0.2166 - accuracy: 0.9107 - val_loss: 0.5463 - val_accuracy: 0.7890\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1be7afc2920>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXBAaquRlff"
      },
      "source": [
        "## Part 4 - Making a single prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-g9zRpC-mYdK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras import utils\n",
        "\n",
        "test_image = utils.load_img('C:/Users/Asus/Desktop/programitas/CatDogClassifier/dataset/dataset/test_set/cats/cat.4034.jpg',target_size = (64,64))\n",
        "test_image = utils.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QL3u-O7OoJTN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat\n"
          ]
        }
      ],
      "source": [
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
